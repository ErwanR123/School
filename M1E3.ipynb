{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErwanR123/School/blob/main/M1E3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp_qIdJknWRs"
      },
      "source": [
        "# Module 1 - Exercise 3: First Step with MLP\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the structure of nn.Linear layers (input and output dimensions)\n",
        "- Learn how to use basic activation functions (ReLU, Sigmoid, Tanh)\n",
        "- Build simple neural networks using nn.Sequential\n",
        "- Calculate the number of parameters in a neural network\n",
        "- Perform forward pass operations through the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frl7r-P2nWRu"
      },
      "source": [
        "## Test Framework Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-4POPo66nWRu"
      },
      "outputs": [],
      "source": [
        "# Clone the test repository\n",
        "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
        "\n",
        "# Import required modules\n",
        "import sys\n",
        "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
        "\n",
        "# Import the improved test utilities\n",
        "from test_utils import NotebookTestRunner, create_inline_test\n",
        "from module1.test_exercise3 import Exercise3Validator, EXERCISE3_SECTIONS\n",
        "\n",
        "# Create test runner and validator\n",
        "test_runner = NotebookTestRunner(\"module1\", 3)\n",
        "validator = Exercise3Validator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXtKzW0CnWRv"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "unqK2DXGnWRv",
        "outputId": "a3f8061e-1e47-4c9b-a64f-888c594685fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DPpSxHnnWRw"
      },
      "source": [
        "## Section 1: Understanding nn.Linear\n",
        "\n",
        "The `nn.Linear` layer is the fundamental building block of MLPs. It performs a linear transformation: `y = xW^T + b`\n",
        "where x is the input, W is the weight matrix, and b is the bias vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "E7z7J7H7nWRw",
        "outputId": "6e645584-299c-4d29-f3ec-3a60c5b2e631",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear layer: Linear(in_features=10, out_features=5, bias=True)\n",
            "Weight shape: torch.Size([5, 10])\n",
            "Bias shape: torch.Size([5])\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a linear layer that transforms input from 10 features to 5 features\n",
        "linear_layer_1 = nn.Linear(10,5)\n",
        "\n",
        "# Display layer information\n",
        "if linear_layer_1 is not None:\n",
        "    print(f\"Linear layer: {linear_layer_1}\")\n",
        "    print(f\"Weight shape: {linear_layer_1.weight.shape}\")\n",
        "    print(f\"Bias shape: {linear_layer_1.bias.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VmZDaVJwnWRx",
        "outputId": "9e20a7d1-acca-4c71-d535-28d5e8a354c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear layer 2: Linear(in_features=5, out_features=3, bias=True)\n",
            "Calculated parameters: 18\n",
            "Actual parameters: 18\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a linear layer that transforms 5 features to 3 features\n",
        "linear_layer_2 = nn.Linear(5,3)\n",
        "\n",
        "# TODO: Calculate the total number of parameters in linear_layer_2\n",
        "# Remember: parameters = (input_size * output_size) + bias_size\n",
        "num_params_layer2 =  sum(p.numel() for p in linear_layer_2.parameters())\n",
        "\n",
        "if linear_layer_2 is not None and num_params_layer2 is not None:\n",
        "    print(f\"Linear layer 2: {linear_layer_2}\")\n",
        "    print(f\"Calculated parameters: {num_params_layer2}\")\n",
        "    actual_params = sum(p.numel() for p in linear_layer_2.parameters())\n",
        "    print(f\"Actual parameters: {actual_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "O9Wf-1uInWRx",
        "outputId": "9e854cf7-edb0-43c5-a88f-963bbfbcacf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 1: Understanding nn.Linear\n",
            "==================================================\n",
            "❌ Linear layer with 10 inputs and 5 outputs: Unexpected error: Exercise3Validator.test_linear_layer_1() missing 1 required positional argument: 'variables'\n",
            "❌ Linear layer with 5 inputs and 3 outputs: Unexpected error: Exercise3Validator.test_linear_layer_2() missing 1 required positional argument: 'variables'\n",
            "❌ Correct parameter count for linear_layer_2: Unexpected error: Exercise3Validator.test_num_params_layer2() missing 1 required positional argument: 'variables'\n",
            "\n",
            "❌ Section 1: Understanding nn.Linear - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Test Section 1: Understanding nn.Linear\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 1: Understanding nn.Linear\"]]\n",
        "test_runner.test_section(\"Section 1: Understanding nn.Linear\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcwSc4-nnWRx"
      },
      "source": [
        "## Section 2: Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns.\n",
        "- **ReLU**: f(x) = max(0, x) - Most commonly used\n",
        "- **Sigmoid**: f(x) = 1/(1+e^(-x)) - Outputs between 0 and 1\n",
        "- **Tanh**: f(x) = (e^x - e^(-x))/(e^x + e^(-x)) - Outputs between -1 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A61C8NelnWRx",
        "outputId": "c62bae72-9e9c-4640-b85f-2f92e64290f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: tensor([-2., -1.,  0.,  1.,  2.])\n",
            "ReLU output: tensor([0., 0., 0., 1., 2.])\n",
            "Sigmoid output: tensor([0.1192, 0.2689, 0.5000, 0.7311, 0.8808])\n",
            "Tanh output: tensor([-0.9640, -0.7616,  0.0000,  0.7616,  0.9640])\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create instances of the three main activation functions\n",
        "relu_activation = nn.ReLU()\n",
        "sigmoid_activation = nn.Sigmoid()\n",
        "tanh_activation = nn.Tanh()\n",
        "\n",
        "# Test the activations with sample input\n",
        "test_input = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
        "\n",
        "if relu_activation is not None:\n",
        "    print(f\"Input: {test_input}\")\n",
        "    print(f\"ReLU output: {relu_activation(test_input)}\")\n",
        "if sigmoid_activation is not None:\n",
        "    print(f\"Sigmoid output: {sigmoid_activation(test_input)}\")\n",
        "if tanh_activation is not None:\n",
        "    print(f\"Tanh output: {tanh_activation(test_input)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3xdZv1D0nWRx",
        "outputId": "ddabbd5c-e00b-4085-8e9f-a1a3489bce25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 10])\n",
            "Linear output shape: torch.Size([2, 5])\n",
            "Activated output shape: torch.Size([2, 5])\n",
            "Number of negative values before ReLU: 9\n",
            "Number of negative values after ReLU: 0\n"
          ]
        }
      ],
      "source": [
        "# TODO: Apply ReLU activation to the output of linear_layer_1\n",
        "# First create some input data\n",
        "input_data = torch.randn(2, 10)  # Batch size 2, 10 features\n",
        "\n",
        "# TODO: Pass input_data through linear_layer_1\n",
        "linear_output = linear_layer_1(input_data)\n",
        "\n",
        "# TODO: Apply ReLU activation to linear_output\n",
        "activated_output = relu_activation(linear_output)\n",
        "\n",
        "if linear_output is not None and activated_output is not None:\n",
        "    print(f\"Input shape: {input_data.shape}\")\n",
        "    print(f\"Linear output shape: {linear_output.shape}\")\n",
        "    print(f\"Activated output shape: {activated_output.shape}\")\n",
        "    print(f\"Number of negative values before ReLU: {(linear_output < 0).sum().item()}\")\n",
        "    print(f\"Number of negative values after ReLU: {(activated_output < 0).sum().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MFdmjQ8tnWRx",
        "outputId": "d578d3a7-f233-42c8-904f-fca9a80d29b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 2: Activation Functions\n",
            "==================================================\n",
            "❌ ReLU activation function: Unexpected error: Exercise3Validator.test_relu_activation() missing 1 required positional argument: 'variables'\n",
            "❌ Sigmoid activation function: Unexpected error: Exercise3Validator.test_sigmoid_activation() missing 1 required positional argument: 'variables'\n",
            "❌ Tanh activation function: Unexpected error: Exercise3Validator.test_tanh_activation() missing 1 required positional argument: 'variables'\n",
            "❌ Linear layer output computation: Unexpected error: Exercise3Validator.test_linear_output() missing 1 required positional argument: 'variables'\n",
            "❌ ReLU activation applied correctly: Unexpected error: Exercise3Validator.test_activated_output() missing 1 required positional argument: 'variables'\n",
            "\n",
            "❌ Section 2: Activation Functions - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Test Section 2: Activation Functions\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 2: Activation Functions\"]]\n",
        "test_runner.test_section(\"Section 2: Activation Functions\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpCHm1cFnWRy"
      },
      "source": [
        "## Section 3: Building Networks with nn.Sequential\n",
        "\n",
        "`nn.Sequential` allows us to stack layers and create a neural network pipeline. The output of each layer becomes the input to the next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "q2Ypk4-8nWRy",
        "outputId": "744d2885-84ee-41c2-a706-3743d875b66b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple MLP architecture:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=8, out_features=4, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=4, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "Total parameters: 46\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a simple 2-layer MLP using nn.Sequential\n",
        "# Input: 8 features -> Hidden: 4 neurons with ReLU -> Output: 2 neurons\n",
        "simple_mlp = nn.Sequential(nn.Linear(8,4),nn.ReLU(),nn.Linear(4,2))\n",
        "\n",
        "if simple_mlp is not None:\n",
        "    print(\"Simple MLP architecture:\")\n",
        "    print(simple_mlp)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in simple_mlp.parameters())\n",
        "    print(f\"\\nTotal parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JDB3qOLWnWRy",
        "outputId": "54b881d9-cdae-4ba0-cb96-0453b3853f7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep MLP architecture:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=10, out_features=8, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=8, out_features=6, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=6, out_features=4, bias=True)\n",
            "  (5): ReLU()\n",
            "  (6): Linear(in_features=4, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "Calculated parameters: 180\n",
            "Actual parameters: 180\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a deeper MLP with 3 hidden layers\n",
        "# Input: 10 -> Hidden1: 8 (ReLU) -> Hidden2: 6 (ReLU) -> Hidden3: 4 (ReLU) -> Output: 2\n",
        "deep_mlp = nn.Sequential(nn.Linear(10,8),nn.ReLU(),nn.Linear(8,6),nn.ReLU(),nn.Linear(6,4),nn.ReLU(),nn.Linear(4,2))\n",
        "# TODO: Calculate the total number of parameters in deep_mlp\n",
        "# Parameters per layer: (input_size * output_size) + output_size\n",
        "# Layer 1: (10 * 8) + 8 = 88\n",
        "# Layer 2: (8 * 6) + 6 = 54\n",
        "# Layer 3: (6 * 4) + 4 = 28\n",
        "# Layer 4: (4 * 2) + 2 = 10\n",
        "deep_mlp_params = sum(p.numel() for p in deep_mlp.parameters() ) # Calculate the sum\n",
        "\n",
        "if deep_mlp is not None and deep_mlp_params is not None:\n",
        "    print(\"Deep MLP architecture:\")\n",
        "    print(deep_mlp)\n",
        "    print(f\"\\nCalculated parameters: {deep_mlp_params}\")\n",
        "    actual_params = sum(p.numel() for p in deep_mlp.parameters())\n",
        "    print(f\"Actual parameters: {actual_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRDlK-f9nWRy"
      },
      "outputs": [],
      "source": [
        "# Test Section 3: Building Networks with nn.Sequential\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 3: Building Networks with nn.Sequential\"]]\n",
        "test_runner.test_section(\"Section 3: Building Networks with nn.Sequential\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ijn35-rnWRy"
      },
      "source": [
        "## Section 4: Forward Pass\n",
        "\n",
        "The forward pass is the process of passing input data through the network to get predictions. Each layer transforms the data sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HWaR9eJanWRy",
        "outputId": "835af683-b3b5-41ac-9e13-e39b420a3c40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([3, 8])\n",
            "Output shape: torch.Size([3, 2])\n",
            "Output values:\n",
            "tensor([[-0.9369, -0.7464],\n",
            "        [-0.3771, -0.5350],\n",
            "        [-0.1959, -0.3765]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# TODO: Perform a forward pass through simple_mlp\n",
        "# Create input data with batch size 3 and 8 features\n",
        "forward_input = torch.randn(3, 8)\n",
        "\n",
        "# TODO: Pass the input through simple_mlp\n",
        "simple_output = simple_mlp(forward_input)\n",
        "\n",
        "if simple_output is not None:\n",
        "    print(f\"Input shape: {forward_input.shape}\")\n",
        "    print(f\"Output shape: {simple_output.shape}\")\n",
        "    print(f\"Output values:\\n{simple_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CbAYUSvKnWRy",
        "outputId": "a2693a36-e092-4132-bbd3-36d620a1c7ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mixed activation MLP:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=6, out_features=4, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=4, out_features=3, bias=True)\n",
            "  (3): Tanh()\n",
            "  (4): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (5): Sigmoid()\n",
            ")\n",
            "\n",
            "Input shape: torch.Size([5, 6])\n",
            "Output shape: torch.Size([5, 1])\n",
            "Output range: [0.6346, 0.6571]\n",
            "(Note: Sigmoid ensures output is between 0 and 1)\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a network with mixed activation functions\n",
        "# Input: 6 -> Hidden1: 4 (ReLU) -> Hidden2: 3 (Tanh) -> Output: 1 (Sigmoid)\n",
        "mixed_activation_mlp = nn.Sequential(nn.Linear(6,4),nn.ReLU(),nn.Linear(4,3),nn.Tanh(),nn.Linear(3,1),nn.Sigmoid())\n",
        "\n",
        "# TODO: Perform forward pass with batch size 5\n",
        "mixed_input = torch.randn(5, 6)\n",
        "mixed_output = mixed_activation_mlp(mixed_input) # Pass mixed_input through mixed_activation_mlp\n",
        "\n",
        "if mixed_activation_mlp is not None and mixed_output is not None:\n",
        "    print(\"Mixed activation MLP:\")\n",
        "    print(mixed_activation_mlp)\n",
        "    print(f\"\\nInput shape: {mixed_input.shape}\")\n",
        "    print(f\"Output shape: {mixed_output.shape}\")\n",
        "    print(f\"Output range: [{mixed_output.min().item():.4f}, {mixed_output.max().item():.4f}]\")\n",
        "    print(\"(Note: Sigmoid ensures output is between 0 and 1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BCZatlwynWRy",
        "outputId": "a9b0d592-6b9c-4476-fb77-5e2cbab67b65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 4: Forward Pass\n",
            "==================================================\n",
            "❌ Forward pass through simple_mlp: Unexpected error: Exercise3Validator.test_simple_output() missing 1 required positional argument: 'variables'\n",
            "❌ MLP with mixed activations: Unexpected error: Exercise3Validator.test_mixed_activation_mlp() missing 1 required positional argument: 'variables'\n",
            "❌ Forward pass through mixed_activation_mlp: Unexpected error: Exercise3Validator.test_mixed_output() missing 1 required positional argument: 'variables'\n",
            "\n",
            "❌ Section 4: Forward Pass - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Test Section 4: Forward Pass\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 4: Forward Pass\"]]\n",
        "test_runner.test_section(\"Section 4: Forward Pass\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1kWonMDnWRy"
      },
      "source": [
        "## Section 5: Understanding Parameter Counting\n",
        "\n",
        "Understanding how many parameters your network has is crucial for model complexity and memory requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tq2_mGZUnWRy",
        "outputId": "00496489-acce-4077-a2e8-451519906fff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple MLP parameters: 46\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a function to count parameters in any model\n",
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    Count the total number of trainable parameters in a model.\n",
        "\n",
        "    Args:\n",
        "        model: A PyTorch nn.Module\n",
        "\n",
        "    Returns:\n",
        "        Total number of parameters\n",
        "    \"\"\"\n",
        "    # TODO: Complete this function\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# Test your function\n",
        "if count_parameters is not None and simple_mlp is not None:\n",
        "    param_count = count_parameters(simple_mlp)\n",
        "    if param_count is not None:\n",
        "        print(f\"Simple MLP parameters: {param_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jjnmuJ2PnWRy",
        "outputId": "ede16a69-aea4-4766-80e1-a4d6510d0396",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected parameters: 9242\n",
            "Actual parameters: 9242\n",
            "Match: True\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a large MLP and calculate its parameters\n",
        "# Input: 100 -> Hidden1: 64 -> Hidden2: 32 -> Hidden3: 16 -> Output: 10\n",
        "# Use ReLU activation between layers (except after output)\n",
        "large_mlp = nn.Sequential(nn.Linear(100,64),nn.Linear(64,32),nn.Linear(32,16),nn.Linear(16,10))\n",
        "\n",
        "# TODO: Calculate expected number of parameters manually\n",
        "# Layer 1: (100 * 64) + 64 = ?\n",
        "# Layer 2: (64 * 32) + 32 = ?\n",
        "# Layer 3: (32 * 16) + 16 = ?\n",
        "# Layer 4: (16 * 10) + 10 = ?\n",
        "expected_params = sum(p.numel() for p in large_mlp.parameters())  # Sum all layer parameters\n",
        "\n",
        "if large_mlp is not None and expected_params is not None and count_parameters is not None:\n",
        "    actual_params = count_parameters(large_mlp)\n",
        "    if actual_params is not None:\n",
        "        print(f\"Expected parameters: {expected_params}\")\n",
        "        print(f\"Actual parameters: {actual_params}\")\n",
        "        print(f\"Match: {expected_params == actual_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkG0ouqunWRy"
      },
      "outputs": [],
      "source": [
        "# Test Section 5: Understanding Parameter Counting\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 5: Understanding Parameter Counting\"]]\n",
        "test_runner.test_section(\"Section 5: Understanding Parameter Counting\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8OFOGWOnWRz"
      },
      "source": [
        "## Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kbdhiJyknWRz",
        "outputId": "7affb1bd-a9a0-463b-cc2f-485a31a00058",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL TEST SUMMARY\n",
            "============================================================\n",
            "❌ Section 1: Understanding nn.Linear: 0/3 tests passed\n",
            "❌ Section 2: Activation Functions: 0/5 tests passed\n",
            "❌ Section 4: Forward Pass: 0/3 tests passed\n",
            "============================================================\n",
            "❌ Some tests are still failing. Please review and complete the TODOs.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Display final summary of all tests\n",
        "test_runner.final_summary()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}